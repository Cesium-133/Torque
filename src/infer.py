import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

import h5py
import numpy as np
import argparse
from pathlib import Path
from joblib import load
import tensorflow as tf
from tqdm import tqdm
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo

def load_and_preprocess_single_file(file_path: Path, scaler, n_frames: int, target_length: int = 150):
    """
    åŠ è½½å•ä¸ªH5æ–‡ä»¶ï¼Œå¹¶åº”ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„é¢„å¤„ç†ï¼ˆå›å½’ä»»åŠ¡ï¼‰ã€‚
    
    Args:
        file_path (Path): H5æ–‡ä»¶è·¯å¾„
        scaler: è®­ç»ƒæ—¶ä¿å­˜çš„æ ‡å‡†åŒ–å™¨
        n_frames (int): è¾“å…¥å†å²å¸§æ•°
        target_length (int): ç›®æ ‡åºåˆ—é•¿åº¦
        
    Returns:
        tuple: (processed_sequences, positions) å¤„ç†åçš„åºåˆ—å’Œä½ç½®ç¼–ç 
    """
    with h5py.File(file_path, 'r') as f:
        if '/right_arm_effort' not in f:
            raise ValueError(f"åœ¨æ–‡ä»¶ {file_path.name} ä¸­æ²¡æœ‰æ‰¾åˆ°æ‰€éœ€çš„æ•°æ®ç»“æ„ã€‚")
        
        # åŠ è½½æ•°æ®
        effort_data = f['/right_arm_effort'][:]  # (seq_len, 7)
    
    # Paddingæˆ–æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
    if len(effort_data) > target_length:
        effort_data = effort_data[:target_length]
    elif len(effort_data) < target_length:
        pad_length = target_length - len(effort_data)
        effort_pad = np.zeros((pad_length, effort_data.shape[1]))
        effort_data = np.concatenate([effort_data, effort_pad], axis=0)
    
    # åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—ç”¨äºæ¨ç†
    sequences = []
    positions = []
    
    seq_len = len(effort_data)
    for start_idx in range(seq_len - n_frames + 1):
        input_window = effort_data[start_idx:start_idx + n_frames]  # (n_frames, n_features)
        sequences.append(input_window)
        positions.append(start_idx)  # ä½ç½®ç¼–ç 
    
    if not sequences:
        raise ValueError(f"åºåˆ—é•¿åº¦ {seq_len} å°äºæ‰€éœ€çš„è¾“å…¥å¸§æ•° {n_frames}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶æ ‡å‡†åŒ–
    sequences = np.array(sequences)  # (n_windows, n_frames, n_features)
    positions = np.array(positions)  # (n_windows,)
    n_windows, n_frames_check, n_features = sequences.shape
    
    # æ ‡å‡†åŒ–
    sequences_reshaped = sequences.reshape(-1, n_features)
    sequences_scaled = scaler.transform(sequences_reshaped)
    sequences_final = sequences_scaled.reshape(n_windows, n_frames_check, n_features)
    
    return sequences_final, positions

def predict_torque_sequence(model, sequences, positions, m_frames=2):
    """
    å¯¹åºåˆ—è¿›è¡Œtorqueå›å½’é¢„æµ‹
    
    Args:
        model: è®­ç»ƒå¥½çš„å›å½’æ¨¡å‹
        sequences: è¾“å…¥åºåˆ— (n_windows, n_frames, n_features)
        positions: ä½ç½®ç¼–ç  (n_windows,)
        m_frames: é¢„æµ‹çš„æœªæ¥å¸§æ•°
        
    Returns:
        predictions: é¢„æµ‹çš„torqueå€¼
    """
    predictions = model.predict([sequences, positions], verbose=0)
    print(f"ğŸ” Debug: model prediction shape = {predictions.shape}")
    print(f"ğŸ” Debug: prediction range - min: {predictions.min():.6f}, max: {predictions.max():.6f}, mean: {predictions.mean():.6f}")
    print(f"ğŸ” Debug: first few predictions = {predictions[:3].flatten()}")
    return predictions

def aggregate_torque_predictions(predictions, method='mean'):
    """
    èšåˆå¤šä¸ªæ—¶é—´çª—å£çš„torqueé¢„æµ‹ç»“æœ
    
    Args:
        predictions: æ¨¡å‹é¢„æµ‹ç»“æœ (n_windows, m_frames)
        method: èšåˆæ–¹æ³• ('mean', 'median', 'last')
        
    Returns:
        aggregated_prediction: èšåˆåçš„é¢„æµ‹ç»“æœ
    """
    if method == 'mean':
        return np.mean(predictions, axis=0)  # å¯¹æ‰€æœ‰çª—å£å–å¹³å‡
    elif method == 'median':
        return np.median(predictions, axis=0)  # å¯¹æ‰€æœ‰çª—å£å–ä¸­ä½æ•°
    elif method == 'last':
        return predictions[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªçª—å£çš„é¢„æµ‹
    else:
        return np.mean(predictions, axis=0)  # é»˜è®¤ä½¿ç”¨å¹³å‡å€¼

def load_truth_values_from_file(file_path: Path, n_frames: int, m_frames: int, target_length: int = 150):
    """
    ä»H5æ–‡ä»¶ä¸­åŠ è½½çœŸå®çš„torqueå€¼ï¼Œç”¨äºå¯è§†åŒ–å¯¹æ¯”
    
    Args:
        file_path (Path): H5æ–‡ä»¶è·¯å¾„
        n_frames (int): è¾“å…¥å†å²å¸§æ•°
        m_frames (int): é¢„æµ‹çš„æœªæ¥å¸§æ•°
        target_length (int): ç›®æ ‡åºåˆ—é•¿åº¦
        æ³¨æ„ï¼šçœŸå®å€¼ä½¿ç”¨åŸå§‹å°ºåº¦ï¼Œä¸è®­ç»ƒæ—¶çš„ç›®æ ‡å€¼ä¿æŒä¸€è‡´
        
    Returns:
        tuple: (truth_values, time_indices) çœŸå®å€¼å’Œå¯¹åº”çš„æ—¶é—´ç´¢å¼•
    """
    with h5py.File(file_path, 'r') as f:
        if '/right_arm_effort' not in f:
            raise ValueError(f"åœ¨æ–‡ä»¶ {file_path.name} ä¸­æ²¡æœ‰æ‰¾åˆ°æ‰€éœ€çš„æ•°æ®ç»“æ„ã€‚")
        
        # åŠ è½½åŸå§‹torqueæ•°æ®
        effort_data = f['/right_arm_effort'][:]  # (seq_len, 7)
    
    # åº”ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†
    if len(effort_data) > target_length:
        effort_data = effort_data[:target_length]
    elif len(effort_data) < target_length:
        pad_length = target_length - len(effort_data)
        effort_pad = np.zeros((pad_length, effort_data.shape[1]))
        effort_data = np.concatenate([effort_data, effort_pad], axis=0)
    
    # ğŸš¨ é‡è¦ä¿®å¤ï¼šè®­ç»ƒæ—¶ç›®æ ‡å€¼y_windowsæ˜¯ä»åŸå§‹æ•°æ®æå–çš„ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    # æ‰€ä»¥æ¨ç†æ—¶çš„çœŸå®å€¼ä¹Ÿåº”è¯¥ä½¿ç”¨åŸå§‹å°ºåº¦ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
    print("ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŸå§‹å°ºåº¦çš„çœŸå®å€¼ï¼Œä¸è®­ç»ƒæ—¶çš„ç›®æ ‡å€¼ä¿æŒä¸€è‡´")
    print(f"ğŸ” Debug: åŸå§‹æ•°æ®èŒƒå›´ - min: {effort_data[:, 0].min():.6f}, max: {effort_data[:, 0].max():.6f}, mean: {effort_data[:, 0].mean():.6f}")
    effort_data_scaled = effort_data  # ä¸è¿›è¡Œæ ‡å‡†åŒ–
    
    # æå–çœŸå®çš„æœªæ¥å€¼ç”¨äºå¯¹æ¯”
    # å¯¹äºæ¯ä¸ªé¢„æµ‹çª—å£ï¼Œæå–å¯¹åº”çš„çœŸå®æœªæ¥m_frameså€¼
    truth_values = []
    time_indices = []
    
    seq_len = len(effort_data_scaled)
    for start_idx in range(seq_len - n_frames + 1):
        # é¢„æµ‹çš„æ—¶é—´ç‚¹ä» start_idx + n_frames å¼€å§‹
        future_start = start_idx + n_frames
        if future_start + m_frames <= seq_len:
            # æå–çœŸå®çš„æœªæ¥m_frameså€¼ï¼ˆåªå–ç¬¬ä¸€ç»´ï¼‰
            truth_future = effort_data_scaled[future_start:future_start + m_frames, 0]  # åªå–ç¬¬ä¸€ç»´
            truth_values.append(truth_future)
            # æ—¶é—´ç´¢å¼•å¯¹åº”é¢„æµ‹çš„æ—¶é—´ç‚¹
            time_indices.append(list(range(future_start, future_start + m_frames)))
        else:
            # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œç”¨é›¶å¡«å……
            remaining_frames = seq_len - future_start
            if remaining_frames > 0:
                truth_future = effort_data_scaled[future_start:seq_len, 0]
                # ç”¨é›¶å¡«å……ä¸è¶³çš„å¸§æ•°
                if m_frames - remaining_frames > 0:
                    truth_future = np.concatenate([truth_future, np.zeros(m_frames - remaining_frames)])
            else:
                truth_future = np.zeros(m_frames)
            truth_values.append(truth_future)
            time_indices.append(list(range(future_start, future_start + m_frames)))
    
    # ç¡®ä¿è¿”å›çš„æ•°ç»„æœ‰æ­£ç¡®çš„å½¢çŠ¶
    truth_values = np.array(truth_values)
    print(f"Debug: truth_values.shape after processing = {truth_values.shape}")
    
    # å½“m_frames=1æ—¶ï¼Œç¡®ä¿å½¢çŠ¶æ˜¯(n_windows, 1)è€Œä¸æ˜¯(n_windows,)
    if truth_values.ndim == 1:
        truth_values = truth_values.reshape(-1, 1)
    
    return truth_values, time_indices

def create_interactive_visualization(truth_values, predictions, time_indices, file_name, output_dir):
    """
    åˆ›å»ºäº¤äº’å¼å¯è§†åŒ–å›¾è¡¨ï¼Œæ˜¾ç¤ºçœŸå®å€¼å’Œé¢„æµ‹å€¼çš„å¯¹æ¯”
    
    Args:
        truth_values: çœŸå®å€¼ (n_windows, m_frames)
        predictions: é¢„æµ‹å€¼ (n_windows, m_frames) æˆ– (n_windows,) å½“m_frames=1æ—¶
        time_indices: æ—¶é—´ç´¢å¼•åˆ—è¡¨
        file_name: æ–‡ä»¶å
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†é¢„æµ‹æ•°æ®çš„ç»´åº¦é—®é¢˜
    print(f"Debug: predictions.shape = {predictions.shape}")
    print(f"Debug: truth_values.shape = {truth_values.shape}")
    
    # ç¡®ä¿é¢„æµ‹æ•°æ®æœ‰æ­£ç¡®çš„å½¢çŠ¶
    if predictions.ndim == 1:
        # å½“m_frames=1æ—¶ï¼Œpredictionså¯èƒ½æ˜¯(n_windows,)ï¼Œéœ€è¦reshapeä¸º(n_windows, 1)
        predictions = predictions.reshape(-1, 1)
    elif predictions.ndim == 2 and predictions.shape[1] == 1:
        # å·²ç»æ˜¯æ­£ç¡®çš„å½¢çŠ¶(n_windows, 1)
        pass
    
    # åˆ›å»ºå›¾è¡¨
    fig = go.Figure()
    
    # æ·»åŠ çœŸå®å€¼çš„å®çº¿
    # å°†æ‰€æœ‰çœŸå®å€¼è¿æ¥æˆä¸€æ¡è¿ç»­çš„çº¿
    all_truth_times = []
    all_truth_values = []
    
    for i, (truth_seq, time_seq) in enumerate(zip(truth_values, time_indices)):
        # ç¡®ä¿truth_seqæ˜¯ä¸€ç»´æ•°ç»„
        if truth_seq.ndim > 1:
            truth_seq = truth_seq.flatten()
        all_truth_times.extend(time_seq)
        all_truth_values.extend(truth_seq)
    
    # å»é‡å¹¶æ’åºï¼Œåˆ›å»ºè¿ç»­çš„çœŸå®å€¼çº¿
    time_truth_pairs = list(zip(all_truth_times, all_truth_values))
    time_truth_pairs = sorted(list(set(time_truth_pairs)))
    unique_times, unique_truths = zip(*time_truth_pairs)
    
    # æ·»åŠ çœŸå®å€¼å®çº¿
    fig.add_trace(go.Scatter(
        x=unique_times,
        y=unique_truths,
        mode='lines',
        name='Truth Values',
        line=dict(color='blue', width=3),
        hovertemplate='Time: %{x}<br>Truth: %{y:.6f}<extra></extra>'
    ))
    
    # æ·»åŠ æ¯ä¸ªé¢„æµ‹çª—å£çš„è™šçº¿
    for i, (pred_seq, time_seq) in enumerate(zip(predictions, time_indices)):
        # å¤„ç†é¢„æµ‹å€¼çš„ç»´åº¦
        if pred_seq.ndim > 1:
            pred_values = pred_seq[:, 0]  # åªå–ç¬¬ä¸€ç»´
        else:
            pred_values = pred_seq if isinstance(pred_seq, np.ndarray) else [pred_seq]
        
        # ç¡®ä¿pred_valueså’Œtime_seqé•¿åº¦åŒ¹é…
        if len(pred_values) != len(time_seq):
            print(f"Warning: pred_values length {len(pred_values)} != time_seq length {len(time_seq)}")
            continue
            
        fig.add_trace(go.Scatter(
            x=time_seq,
            y=pred_values,
            mode='lines+markers',  # æ·»åŠ markersä½¿å•ç‚¹æ›´æ˜æ˜¾
            name=f'Prediction Window {i+1}',
            line=dict(dash='dot', width=2, color=f'rgba(255, 0, 0, 0.7)'),
            marker=dict(size=4, color='red'),
            hovertemplate=f'Window {i+1}<br>Time: %{{x}}<br>Prediction: %{{y:.6f}}<extra></extra>',
            showlegend=(i == 0)  # åªåœ¨ç¬¬ä¸€æ¡é¢„æµ‹çº¿æ˜¾ç¤ºå›¾ä¾‹
        ))
    
    # è®¾ç½®å›¾è¡¨å¸ƒå±€
    fig.update_layout(
        title=f'Torque Prediction Visualization - {file_name}',
        xaxis_title='Time Step',
        yaxis_title='Torque Value (First Dimension)',
        hovermode='closest',
        legend=dict(x=0.02, y=0.98),
        width=1200,
        height=600,
        template='plotly_white'
    )
    
    # ä¿å­˜äº¤äº’å¼HTMLæ–‡ä»¶
    html_file = output_path / f'{file_name}_visualization.html'
    fig.write_html(html_file)
    
    print(f"äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜è‡³: {html_file}")
    return html_file

def main(args):
    # --- 1. è§£ææ¨¡å‹å‚æ•° ---
    model_path = Path(args.model_path)
    input_dir = Path(args.input_dir)
    output_file = Path(args.output_file)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ä»æ¨¡å‹æ–‡ä»¶åè§£æå‚æ•°
    model_name_stem = model_path.stem
    try:
        # è§£ææ¨¡å‹æ–‡ä»¶åæ ¼å¼ï¼šmodel_type_regressor_nX_mY_lenZ_foldW_model æˆ– model_type_regressor_nX_mY_lenZ_model
        parts = model_name_stem.split('_')
        
        # æŸ¥æ‰¾n_frameså’Œm_frameså‚æ•°
        n_frames = None
        m_frames = None
        target_length = None
        
        for part in parts:
            if part.startswith('n') and part[1:].isdigit():
                n_frames = int(part[1:])
            elif part.startswith('m') and part[1:].isdigit():
                m_frames = int(part[1:])
            elif part.startswith('len') and part[3:].isdigit():
                target_length = int(part[3:])
        
        if n_frames is None or m_frames is None or target_length is None:
            raise ValueError("æ— æ³•è§£ææ¨¡å‹å‚æ•°")
            
    except (IndexError, ValueError) as e:
        print(f"\033[91mé”™è¯¯: æ— æ³•ä»æ¨¡å‹æ–‡ä»¶å '{model_path.name}' ä¸­æ¨æ–­å‚æ•°ã€‚\033[0m")
        print(f"æ–‡ä»¶ååº”éµå¾ªæ ¼å¼: 'modeltype_regressor_nX_mY_lenZ_model.h5'")
        return

    # æ„å»ºscalerè·¯å¾„
    scaler_name = model_name_stem.replace('_model', '_scaler.joblib')
    # å¦‚æœæ˜¯foldæ¨¡å‹ï¼Œéœ€è¦ç§»é™¤foldä¿¡æ¯æ¥æ‰¾åˆ°å¯¹åº”çš„scaler
    scaler_name = '_'.join([part for part in scaler_name.split('_') if not part.startswith('fold')])
    scaler_path = model_path.parent / scaler_name

    if not model_path.exists() or not scaler_path.exists():
        print(f"\033[91mé”™è¯¯: æ¨¡å‹æˆ–Scaleræœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚\næ¨¡å‹: {model_path}\nScaler: {scaler_path}\033[0m")
        return

    print(f"æ¨¡å‹å‚æ•°: n_frames={n_frames}, m_frames={m_frames}, target_length={target_length}")

    # --- 2. åŠ è½½æ¨¡å‹å’ŒScaler ---
    print(f"æ­£åœ¨åŠ è½½å›å½’æ¨¡å‹: {model_path}")
    # è§£å†³Kerasç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼šæ˜¾å¼æŒ‡å®šè‡ªå®šä¹‰å¯¹è±¡
    custom_objects = {
        'mse': tf.keras.losses.MeanSquaredError(),
        'mae': tf.keras.metrics.MeanAbsoluteError()
    }
    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
    print(f"æ­£åœ¨åŠ è½½Scaler: {scaler_path}")
    scaler = load(scaler_path)

    # --- 3. æŸ¥æ‰¾æ‰€æœ‰å¾…æ¨ç†çš„æ–‡ä»¶ ---
    h5_files_to_infer = list(input_dir.rglob('*.h5')) + list(input_dir.rglob('*.hdf5'))
    if not h5_files_to_infer:
        print(f"\033[93måœ¨ç›®å½• '{input_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .h5 æˆ– .hdf5 æ–‡ä»¶ã€‚\033[0m")
        return
    
    print(f"æ‰¾åˆ° {len(h5_files_to_infer)} ä¸ªæ–‡ä»¶ã€‚å¼€å§‹æ‰¹é‡torqueå›å½’æ¨ç†...")

    # --- 4. æ‰¹é‡æ¨ç†å¹¶ä¿å­˜ç»“æœ ---
    with open(output_file, 'w', encoding='utf-8') as f_out:
        # å†™å…¥å¤´éƒ¨ä¿¡æ¯
        f_out.write(f"# GRU Torque Regression Results\n")
        f_out.write(f"# Model: {model_path.name}\n")
        f_out.write(f"# Parameters: n_frames={n_frames}, m_frames={m_frames}\n")
        f_out.write(f"# Aggregation Method: {args.aggregation_method}\n")
        f_out.write(f"# Format: filepath - predicted_torque_values\n\n")
        
        for file_path in tqdm(h5_files_to_infer, desc="Inferring"):
            try:
                # 4.1 é¢„å¤„ç†å•ä¸ªæ–‡ä»¶
                sequences, positions = load_and_preprocess_single_file(
                    file_path, scaler, n_frames, target_length
                )
                
                # 4.2 è¿›è¡Œtorqueé¢„æµ‹
                predictions = predict_torque_sequence(model, sequences, positions, m_frames)
                
                # 4.3 èšåˆé¢„æµ‹ç»“æœ
                aggregated_prediction = aggregate_torque_predictions(predictions, args.aggregation_method)
                
                # 4.4 æ ¼å¼åŒ–è¾“å‡º
                torque_values = [f"{val:.6f}" for val in aggregated_prediction]
                torque_str = "[" + ", ".join(torque_values) + "]"
                
                # 4.5 å‡†å¤‡è¾“å‡ºè¡Œ
                output_line = f"{file_path.resolve()} - {torque_str}\n"
                f_out.write(output_line)
                
                # å¦‚æœéœ€è¦è¯¦ç»†è¾“å‡ºï¼Œä¹Ÿå¯ä»¥ä¿å­˜æ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
                if args.detailed_output:
                    f_out.write(f"  Detailed predictions for {file_path.name}:\n")
                    for i, pred in enumerate(predictions):
                        pred_values = [f"{val:.6f}" for val in pred]
                        pred_str = "[" + ", ".join(pred_values) + "]"
                        f_out.write(f"    Window {i}: {pred_str}\n")
                    f_out.write("\n")

                # 5. åŠ è½½çœŸå®å€¼å¹¶è¿›è¡Œå¯è§†åŒ–
                if args.visualize:
                    truth_values, time_indices = load_truth_values_from_file(
                        file_path, n_frames, m_frames, target_length
                    )
                    create_interactive_visualization(
                        truth_values, predictions, time_indices, file_path.stem, args.output_dir
                    )

            except Exception as e:
                error_line = f"{file_path.resolve()} - ERROR: {e}\n"
                f_out.write(error_line)
                tqdm.write(f"\033[91må¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}\033[0m")

    print("\n" + "="*50)
    print("      GRU Torqueå›å½’æ¨ç†å®Œæˆ")
    print("="*50)
    print(f"ç»“æœå·²ä¿å­˜è‡³: {output_file.resolve()}")
    print("="*50)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Perform GRU-based torque regression inference on H5 files.")
    
    parser.add_argument("--model_path", type=str, required=True,
                        help="Path to the saved Keras regression model file (e.g., 'checkpoints/gru_regressor_n10_m2_len150_model.h5').")
    parser.add_argument("--input_dir", type=str, required=True,
                        help="Path to the directory containing H5 files for inference.")
    parser.add_argument("--output_file", type=str, default="output/torque_inference_results.txt",
                        help="Path to save the torque inference results.")
    parser.add_argument("--aggregation_method", type=str, default="mean", 
                        choices=["mean", "median", "last"],
                        help="Method to aggregate multiple window predictions.")
    parser.add_argument("--detailed_output", action="store_true",
                        help="Include detailed predictions for each time window.")
    parser.add_argument("--visualize", action="store_true",
                        help="Generate interactive visualization HTML files.")
    parser.add_argument("--output_dir", type=str, default="output",
                        help="Directory to save visualization HTML files.")

    args = parser.parse_args()
    main(args)